## 大綱
- 從圖譜實體中找尋最相似詞
    1. `fuzzychinese` 計算相似度
    2. `Word2Vec` 找最相似詞
    3. `BERT Embeddings` 拆解後計算距離找最相似
- 合併檢驗
- 自動去重流程

---

## 從圖譜實體中找尋最相似詞
###### `完成`

### [`fuzzychinese`](https://github.com/znwang25/fuzzychinese) 計算相似度 [V]
fuzzychinese 是使用部首、筆畫來做相似詞模糊比對

- 比較適合用在專有名詞的比對
  > 就算是別名也會有重複的字符，例如：{台積電,台積,台灣積體電路}
- 怎樣叫做非專有名詞？
  > 字符完全不同也代表一樣意思，例如：{入侵,攻打}


- 動詞
![](https://i.imgur.com/EsY3nql.png)

- 實體
![](https://i.imgur.com/54aXlN8.png)

fuzzychinese 的壞處：
- 像是筆畫接近的{金山,釜山}竟然被當成最相似詞了
- 因此需要語意輔助，例如金山和釜山的 BERT Embeddings 一定不會是接近的

---

### Word2Vec 找最相似詞 [X]

Word2Vec
- 原本 Word2Vec 做法是將各個文本分詞後陣列輸入模型，針對每個分詞建立空間向量

實體去重作法
- 在這個 case 想以實體為主角，因此改將事件三元組中的實體陣列＋事件觸發詞（動詞）輸入模型。

結果：因為參雜了多種不同實體，找相似詞的效果不佳
![](https://i.imgur.com/YVT6HQD.png)

---

### BERT Embeddings 拆解後計算距離找最相似詞 [V]

- 使用模型：[RoBERTa-wwm-ext-large, Chinese](https://github.com/ymcui/Chinese-BERT-wwm)

![](https://i.imgur.com/X33Pddo.png)

> Google 官方發布的 `BERT-base, Chinese` 預訓練模型中，中文是以字為粒度進行切分，沒有考慮到傳統 NLP 中的中文分詞（CWS）。 BERT-wwm 系列模型將全詞 Mask 的方法應用在了中文中，使用了中文維基百科（包括簡體和繁體）進行訓練，並且使用了哈工大 LTP 作為分詞工具，即對組成同一個詞的漢字全部進行 Mask。

#### 比較使用 BERT Embeddings 不同 Pooling Strategy  區分相似詞的效果

在命名實體識別任務時，對 BERT Embeddings 使用不同 Pooling Strategy，所得到的 F1 Score 結果各有優劣：

![](https://i.imgur.com/GsFoqw8.png)

> Ref:
> - https://colab.research.google.com/drive/1cJzJpGpIk8yXY2Bj7mjmpZK02o7WEZM_?authuser=3#scrollTo=P3D5qnRNmq5_
> - https://jalammar.github.io/illustrated-bert/

我用 2 個簡單句子測試一下，在相似詞上的應用

- text1 = "俄羅斯攻打烏克蘭"
- text2 = "俄國入侵烏國"


先將句子用 `BERT-wwm-ext-large` 的 eval mode 轉為句向量

![](https://i.imgur.com/SjB0cfv.png)

![](https://i.imgur.com/yOFMt2B.png)

經過一些轉換後，將詞向量從句向量中切出

接著計算詞彙倆倆之間的相似度

想測試在相同意思的句型下，同義不同字的詞相似度是否如預期

![](https://i.imgur.com/itOty3V.png)

- 俄羅斯 vs 俄國
- 烏克蘭 vs 烏國
- 攻打 vs 入侵
- 烏克蘭 vs 入侵
- 烏克蘭 vs 俄國

(1) 只使用最後一層 (last hidden state)

![](https://i.imgur.com/gdiyh7V.png)

(2) torch.sum 所有 hidden states

![](https://i.imgur.com/TB6o1ia.png)

可以發現「烏克蘭 vs 入侵」在法(2)有明顯降低，以統一 threshold 來說，法(2)比較貼合我們的目標


#### 將實體區間標出後，計算空間中詞彙兩兩間的距離（分群前一步）

1. 取出完整事件句子及實體位置，轉換為 BERT Word Embeddings
2. 針對每個實體的 Word Embeddings 計算距離


##### 一開始先跑分群

- 將不同事件句子中的重複實體向量先平均到一起，再與其他的實體向量計算距離
  - 倘若沒有先處理重複實體，會造成無效分群，很容易都在分這些重複的結果

  ![](https://i.imgur.com/ZmUM1qH.png)
  
  ![](https://i.imgur.com/2DFNTjL.png)

- 事先合併平均重複實體後再分群，會發現有一大群很 general 的實體被分到同一群
  - 如果要一直針對大群再遞迴分群，會沒有一個指標決定怎樣要停止（不希望人工檢查）

![](https://i.imgur.com/eyGqpMf.png)



> ### 改成只計算距離

最有可能合併的一定是最接近的那個實體，不管還有沒有其他實體被分到同一群。

- 使用 [`scipy`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist) 調用不同指標計算距離
```python=
distance_Y = pdist(_vectors, 'euclidean')
```
![](https://i.imgur.com/IJMjdCM.png)

![](https://i.imgur.com/2W15H9W.png)


`查詢實體 | 在所有實體中取出距離最短的實體 | 兩個實體間的距離`

![](https://i.imgur.com/Flk1zfS.png)

---


## 合併檢驗
###### `實驗中`

怎樣決定要不要合併？
- fuzzychinese 和 BERT Embeddings 同時達到 threshold?

## 自動去重流程
###### `實驗中`

1. 將新詞彙加入，重新計算空間中的距離（想要讓考慮到的詞彙越來越完整）
2. 跑 fuzzychinese + BERT Eebeddings 向量距離 取最相似結果，若重合則合併


- Hackmd
https://hackmd.io/rXsTv81wSaKn5LPMYQgnTg